# -*- coding: utf-8 -*-
"""Actividad3.VAV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-DOqg-Qr4xMmXvyHtIXf0DIMl-VWDIaF

<div style="background-color:#e6f2ff; padding:20px; border-radius:10px;">
<img style="float:left; margin-right:28px; border-radius:8px; box-shadow:0 2px 8px #00004733;" src='alinco.png' width="120"/>
<div style="margin-left:150px;">
<h1 style="color:#000047; font-size:2.3em; margin-bottom:0;">Actividad 3</h1>
<h2 style="color:#003366; font-size:1.3em; margin-top:0;">Tratamiento de datos Faltantes Práctica</h2>
</div>
<br style="clear:both"/>
</div>

<div style="border-left:6px solid #000047; padding:18px; margin-top:18px; background:#f5f5f5; border-radius:8px;">
<span style="font-size:1.1em;"><b>Objetivo:</b>
Practicar los métodos vistos en clase para el tratamiento de datos faltantes.</span>
<ul style="margin-top:10px;">
<li>Procesamiento de datos nulos.</li>
<li>Identificar y tratar valores faltantes.</li>
</ul>
</div>

<div style="border-left:6px solid #003366; border-radius:8px; padding:16px; margin-bottom:16px;">
<h2 style="color:#003366; margin-top:0;">Ejercicio 1</h2>

</div>

El dataset adult.csv fue extraído por Barry Becke de la base de datos de Censos de  1994. Contiene tanto variables numéricas como categóricas. La información de las variables y su contenido se presentan a continuación:

**age:** continuo

**workclass:** Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.

**fnlwgt:** continuo.

**education:** Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool.

**education-num:** continuo.

**marital-status:** Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.

**occupation:** Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.

**relationship:** Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.

**race:** White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.

**sex:** Female, Male.

**capital-gain:** continuo.

**capital-loss:** continuo.

**hours-per-week:** continuo.

**native-country:** United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.
"""

import pandas as pd
from matplotlib import pyplot as plt
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""* Información General del Dataset adult.csv"""

df = pd.read_csv('adult.csv')

df.head(5)

df.info()



dfcopy=df.copy()
dfcopy=dfcopy.isna()
dfcopy.head(20)

"""**1.1- Encontrar el número de valores faltantes por columna**"""

dfcopy = df.copy()
dfcopy = dfcopy.isna()
dfcopy.sum()

"""**1.2.- Encontrar el porcentaje de información faltante**"""

fal_portotal = dfcopy.mean().mean() * 100
fal_porcol = dfcopy.mean() * 100
print(f'Porcentaje de Información Faltante por Columna:\n{fal_porcol}. \n Porcentaje de Información Faltante en Total: {fal_portotal:.2f}.')

"""#### Ignorar fila de datos

**1.3.- El comando `dropna()` permite eliminar las filas y/o columnas en las que hayan datos faltantes. Cree una copia del dataset (`df_copy`) y elimine las filas y/o columnas de `df_copy` y el dataframe resultante guardelo en `df_filtered`.**
"""

df_copy=df.copy()

#df_copy = df_copy.replace(["", " ", "NaN", "None", "?", "n/a"], np.nan)

df_copy.isna().sum()

#Eliminación de filas y columnas  en las que hay datos faltantes
df_filtered = df_copy.dropna().dropna(axis=1)

df_filtered.shape

df_filtered.isna().sum()

"""**df_copy total de 48842 rows 15 columns después de dropna() en filas y columnas, quedaron 45222 rows × 15 columns**"""

df_filtered.info()

df_filtered.sum() #corroboramos que no hay faltantes

"""**1.4.- Note que los índices no cambian. Lo que realiza es la eliminación de la fila (por ejemplo la 14 no está), pero mantiene la indexación. Por tanto, estos no coinciden con el número total de filas. Para reasignar los índices se puede hacer uso del comando `reset_index(drop=True, inplace=True)`. Realice un reseteo del índice de df_filtered.**"""

df_filtered = df_filtered.reset_index(drop=True)

df_filtered

"""**1.5.- Se puede usar el argumento `subset` en el método dropna para seleccionar solo las columnas sobre las que se desea analizar si hay valores nulos para eliminar las filas. Elimine los datos nulos de `df_copy` para la columna: native-country**"""

#Eliminación de filas, sólo si la columna (native-country) tiene datos faltantes
df_filtered = df_copy.dropna(subset=['native-country'])

df_filtered

"""**Si hubo borrado de filas de 488242 quedaron 47985**"""

df_filtered.isna().sum() #Comprobación: en la columna native-country no hay datos faltantes

"""### Llenado con valores vecinos

**1.6.-El comando `fillna()` tiene el argumento `method` que permite hacer un llenado hacia atrás ('bfill') o hacia adelante ('ffill'). Utilice `ffill` y `bfill` con el método `fillna` para el dataframe `df_copy` de los índices 25 hasta 30. Tenga en cuenta que al usar ésta opción si el dato con el que se intenta hacer el llenado también es un NaN, éste permanece.**
"""

df_copy.loc[25:30]

df_copy.loc[25:30] = (df_copy.loc[25:30].fillna(method='ffill').fillna(method='bfill'))

df_copy[25:30]

"""* Para evitar el warning, debe usarse:
df_copy.loc[25:30] = (df_copy.loc[25:30].ffill().bfill())

"""

df_copy.loc[25:30] = (df_copy.loc[25:30].ffill().bfill())

df_copy[25:30]

"""### Llenado de datos usando sklearn

**1.7.- Es posible también usar el metodo 'SimpleImputer' de la libreria sklearn para hacer llenado, éste nos permite definir cual estrategia usar para el llenado de los datos (media, mediana o moda). importe la clase `SimpleImputer` de `sklearn.impute`. Aplique el llenado de los datos nulos mediante la estrategia 'most_frequent'**
"""

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from sklearn.impute import SimpleImputer, KNNImputer

df_copy.isna().sum()



# Asumimos que 'hya' tenía como propósito extraer las columnas categóricas.
# Extraeremos directamente las columnas categóricas usando pandas.
df_categoricos = df_copy.select_dtypes(include='object')
display(df_categoricos.head())

mas_frecuente = SimpleImputer(strategy='most_frequent')
df_mas_frecuente = pd.DataFrame(mas_frecuente.fit_transform(df_categoricos), columns=df_categoricos.columns)

print("Valores faltantes después de imputar variables categóricas por la moda:")
print(df_mas_frecuente.isnull().sum())

# Después de la imputación por la moda, df_mas_frecuente ya contiene los datos categóricos procesados.
display(df_mas_frecuente)
# Comprobación de la aplicación en el llenado de los datos nulos mediante la estrategia 'most_frequent'
# fila 48838

"""<div style="border-left:6px solid #003366; border-radius:8px; padding:16px; margin-bottom:16px;">
<h2 style="color:#003366; margin-top:0;">Ejercicio 2</h2>

</div>

Trataremos un ejemplo de una base de datos de una tabla de características de marcas de carros.
"""

archivo = "https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/auto.csv"

headers = ["symboling","normalized-losses","make","fuel-type","aspiration", "num-of-doors","body-style",
         "drive-wheels","engine-location","wheel-base", "length","width","height","curb-weight","engine-type",
         "num-of-cylinders", "engine-size","fuel-system","bore","stroke","compression-ratio","horsepower",
         "peak-rpm","city-mpg","highway-mpg","price"]

"""##### Carga de datos desde el url

Para esto usamos el método `read_csv()`
"""

df = pd.read_csv(archivo, names = headers)

#df = pd.read_csv('auto.csv', names = headers) se descargo el archivo por si hay problema durante la ejecución

df

"""**2.1.- Inspección del archivo. Recordemos que podemos usar el método head().**"""

df.head()

"""**Podemos observar que muchas de las entradas están etiquetadas con el símbolo `?`. Esto corresponde datos faltantes que pueden dificultar un análisis posterior.**

Los pasos que debemos seguir a continuación son:

> 1.   Identificar los valores faltantes
> 2.   Tratar los valores faltantes.
> 3.   Corregir el formato de los datos.

Usa tu librería para obtener esta información.

#### Identificación de valores faltantes y tratamiento.

**2.2.-Conversión de `?` a `NaN`. Quienes llenaron esta tabla, identificaron los valores faltantes como `?`.  Por razones de tiempo de computo y conveniencia, replazaremos estos signos de interrogación por `NaN`, para lo cual debemos importantar antes la librería numéricas de python numpy. Realice el reemplazo del caracter `?` por np.nan.**

**Hint: puede hacer uso de la librería de clase**
"""

import pandas as pd
import numpy as np
import seaborn as sns

df = df.replace(["?"], np.nan)

df # En el DataFrame df se observa que ya no exist "?" sino NaN

"""**2.3.- Identificación y conteo de valores faltantes. Aplique el método `isnull()` y utilice la función de agregación `sum()` guarde este onbjeto tipo serie en la variable `datos_faltantes`.- Comente cuales son las columnas con el número y porcentaje de datos faltantes**"""

df.isna()

datos_faltantes = df.isnull().sum()
datos_faltantes

por_datos_faltantes = df.isnull().mean() * 100
por_datos_faltantes

resumen_faltantes = pd.DataFrame({
    'Datos_faltantes': datos_faltantes,
    'Porcentaje_faltantes (%)': por_datos_faltantes
})
resumen_faltantes['Datos_faltantes']

cols_con_faltantes = resumen_faltantes[resumen_faltantes['Datos_faltantes'] > 0]
cols_con_faltantes

"""7 de 26 columnas presentan datos faltantes.
La columna con mayor % datos faltantes es normalized-losses.

#### Tratamiento de los datos faltantes

Solo debemos eliminar una columna si la mayoria de entradas son vacias. En nuestro caso, ninguna de las columnas cumplen con este criterio como para ser eliminadas.

En este caso tenemos cierta libertad para aplicar diferences métodos de llenado de datos faltantes; no obstante, algunos métodos pueden resultar más adecuados que otros. Aplique los siguientes métodos para cada columna:

**2.4.- Reemplazo por la media:**
<ul>
    <li>"normalized-losses"</li>
    <li>"stroke"</li>
    <li>"bore"</li>
    <li>"horsepower"</li>
    <li>"peak-rpm"</li>
</ul>

**2.5.- Replace por la moda:**
<ul>
    <li>"num-of-doors"
        <ul>
            <li>Razón: 84% de los sedan son 4 puertas. Dado que los carros cuatro puertas son los más frecuentes, la probabilidad de ocurrencia es mayor.</li>
        </ul>
    </li>
</ul>

**2.5.- Eliminación de una fila entera:**
<ul>
    <li>"price": 4 datos faltantes
        <ul>
            <li>Razón: Como, eventualmente, el precio es algo que se puede predecir, ninguna entrada sin precio puede ser usada para una predicción; por ende, cualquier fila sin precio no será útil.</li>
        </ul>
    </li>
</ul>
"""

# 2.4.- Reemplazo por la media:
mean_cols = ['normalized-losses', 'stroke', 'bore', 'horsepower', 'peak-rpm']
for col in mean_cols:
    # Convertir a numérico antes de calcular la media, forzando errores a NaN
    df[col] = pd.to_numeric(df[col], errors='coerce')
    mean_value = df[col].astype('float').mean()
    df[col].replace(np.nan, mean_value, inplace=True)

# 2.5.- Reemplazo por la moda:
# 'num-of-doors'
most_frequent_num_of_doors = df['num-of-doors'].mode()[0]
df['num-of-doors'].replace(np.nan, most_frequent_num_of_doors, inplace=True)

# 2.5.- Eliminación de filas enteras con valores faltantes en 'price'
# Convertir 'price' a numérico antes de la eliminación para asegurar que NaN sea reconocido
df['price'] = pd.to_numeric(df['price'], errors='coerce')
df.dropna(subset=['price'], axis=0, inplace=True)

# Resetear el índice después de eliminar filas
df.reset_index(drop=True, inplace=True)

print("Valores faltantes después del tratamiento:")
print(df.isnull().sum())

"""#### Corrección de datos

**El último paso en el preprocesado de los datos consiste en revisar de que todos los datos estén en el formato adecuado(int, float, text u otro).**

**Podemos usar en Pandas**
<p><b>.dtypes()</b>  para ver el tipo</p>
<p><b>.astype()</b> para cambiar el tipo</p>

**2.6.- obtenga la lista de tipos de dato del dataframe**
"""

# Importar librerías necesarias
import pandas as pd

# URL del archivo CSV
url = "https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/auto.csv"

# Cargar el archivo en un DataFrame
df = pd.read_csv(url)

# Mostrar los primeros registros para verificar
print(df.head())

# Obtener la lista de tipos de datos de cada columna
print("\nTipos de datos por columna:")
print(df.dtypes)

"""**2.7.- Se puede observa que algunas columnas no tienen el tipo correcto. Las variables numéricas deben ser de tipo 'float' o 'int', y las variables con caracteres, como categorias, deben ser del tipo 'object'. Por ejemplo, 'bore' (diámetro del cilindro) y 'stroke' (ciclos/tiempos) son variables numéricas que describen el motor, esperamos entonces que sean de tipo 'float' o 'int'; no obstante, son de tipo 'object'. convierta los tipos de datos a un tipo adecuado usantdo el método "astype()".**

"""

import pandas as pd
import numpy as np

# URL del archivo CSV
url = "https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DA0101EN/auto.csv"

# Headers para el archivo CSV (copia de la definición en la celda llFJjCx-cMPb)
headers = ["symboling","normalized-losses","make","fuel-type","aspiration", "num-of-doors","body-style",
         "drive-wheels","engine-location","wheel-base", "length","width","height","curb-weight","engine-type",
         "num-of-cylinders", "engine-size","fuel-system","bore","stroke","compression-ratio","horsepower",
         "peak-rpm","city-mpg","highway-mpg","price"]

# Cargar el archivo en un DataFrame, utilizando los headers
df = pd.read_csv(url, names=headers)

# Ver tipos originales
print("Tipos de datos originales:\n", df.dtypes, "\n")

# 1) Reemplazar los marcadores de faltantes "?" por NaN
df = df.replace("?", np.nan)

# 2) Definir columnas que deben ser numéricas (float o int)
numeric_cols = [
    "normalized-losses", "wheel-base", "length", "width", "height",
    "curb-weight", "engine-size", "bore", "stroke", "compression-ratio",
    "horsepower", "peak-rpm", "city-mpg", "highway-mpg", "price"
]

# 3) Convertir a numérico y luego ajustar el tipo con astype()
for col in numeric_cols:
    # Primero convertimos el texto a número (coerce pone NaN si no puede)
    df[col] = pd.to_numeric(df[col], errors="coerce").astype("float")

# 4) (Opcional) Si prefieres enteros para columnas que conceptualmente son enteras:
int_like_cols = ["normalized-losses", "engine-size", "horsepower", "peak-rpm",
                 "city-mpg", "highway-mpg", "curb-weight", "price"]
for col in int_like_cols:
    # Solo convertir a int si no hay NaN; si hay NaN, mantener float para no perderlos
    if df[col].isna().any():
        # Mantener float para no causar error; podrías imputar antes si lo deseas
        df[col] = df[col].astype("float")
    else:
        df[col] = df[col].astype("int")

# 5) Asegurar que columnas categóricas sean de tipo object
object_cols = [
    "symboling", "make", "fuel-type", "aspiration", "num-of-doors",
    "body-style", "drive-wheels", "engine-location", "engine-type",
    "num-of-cylinders", "fuel-system"
]
for col in object_cols:
    if col in df.columns:
        df[col] = df[col].astype("object")

# 6) Mostrar tipos finales
print("Tipos de datos después de conversión:\n", df.dtypes, "\n")

# 7) (Opcional) Vista rápida para verificar
print(df[numeric_cols + object_cols].head())

