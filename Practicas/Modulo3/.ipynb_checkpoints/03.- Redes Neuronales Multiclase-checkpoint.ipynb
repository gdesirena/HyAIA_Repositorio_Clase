{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#000047; padding: 30px; border-radius: 10px; color: white; text-align: center;\">\n",
    "    <img src='Figures/alinco.png' style=\"height: 100px; margin-bottom: 10px;\"/>\n",
    "    <h1>Redes Neuronales Multiclase</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "Los clasificadores no solo trabajan sobre dos clases, es muy común encontrarnos con problemas con más de dos. Para adaptar una red neuronal a un problema de $n$ clases basta con colocar en la capa de salida $n$ neuronas. \n",
    "\n",
    "<img src=\"Figures/multiclass.svg\" width=\"60%\">\n",
    "\n",
    "### Clasificación de Plantas Iris\n",
    "Vamos a estudiar esto haciendo uso de un *dataset* muy conocido utilizado para clasificar el género de plantas iris. La ﬂor de iris se presenta en tres especies distintas denominadas: \n",
    "\n",
    "- setosa\n",
    "- versicolor\n",
    "- virgínica\n",
    "\n",
    "Pueden identiﬁcarse atendiendo a los anchos y altos de sus pétalos y sépalos. Este conjunto de datos contiene 150 muestras de estas ﬂores, recogiendo 50 muestras de cada especie. \n",
    "\n",
    "\n",
    "**Las cuatro primeras columnas corresponden al alto y ancho del sépalo y al alto y ancho del pétalo. La quinta columna indica la especie a la que pertenece:** \n",
    "\n",
    "- 0 → setosa, \n",
    "- 1 → versicolor\n",
    "- 2 → virgínica. \n",
    "\n",
    "\n",
    "<img src=\"Figures/iris.png\" width=\"25%\">\n",
    "\n",
    "\n",
    "Vemos que la quinta columna contiene los valores 0,1 y 2. Son las etiquetas correspondientes a las tres clases. Decíamos que la forma de adaptar ahora una red para clasificar tres clases es poner en la capa de salida tres neuronas. Recordemos que la función de activación que estamos usando es la sigmoide, cuyo rango es $(0,1)$. ¿Cómo hacemos encajar esto para que podamos definir una función de error sobre la cual podamos descender por el gradiente hasta un mínimo?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m X = iris.data\n\u001b[32m      4\u001b[39m y = iris.target\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m data_iris = \u001b[43mpd\u001b[49m.DataFrame(data=X, columns = iris.feature_names)\n\u001b[32m      6\u001b[39m data_iris[\u001b[33m'\u001b[39m\u001b[33mespecie\u001b[39m\u001b[33m'\u001b[39m] = y\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "data_iris = pd.DataFrame(data=X, columns = iris.feature_names)\n",
    "data_iris['especie'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iris['out'] = data_iris.iloc[:,4].astype('category').cat.codes\n",
    "\n",
    "data_iris['out'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Codificación one-hot\n",
    "\n",
    "La solución está en utilizar la codificación **one-hot!!**. \n",
    "\n",
    "Esta codificación hace lo siguiente, se toma un vector de una dimensión igual al número de clases que haya, en este caso, 3. Todas las componentes del vector estarán a 0, excepto una que tendrá el valor 1 y que corresponderá con la clase que defina. Es decir:\n",
    "\n",
    "- 0 →[1,0,0], \n",
    "- 1 →[0,1,0] , \n",
    "- 2 →[0,0,1]. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def one_hot(x, n):\n",
    "    if type(x)==list:\n",
    "        x = np.array(x)\n",
    "    x = x.flatten()\n",
    "    o_h = np.zeros((len(x), n))\n",
    "    o_h[np.arange(len(x)), x] = 1\n",
    "    return o_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randint(0,10,10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot(x, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0,1,2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot(a, n=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Red de clasifiación\n",
    "\n",
    "Definamos ahora la red que vamos a utilizar para nuestro clasificador. Se compondrá de: una capa de entrada con 4 entradas (ancho y alto de pétalo y sépalo), 5 neuronas en la capa oculta y 3 neuronas en la capa de salida, dado que hay tres clases. *¿Y por qué cinco neuronas en la capa oculta?*\n",
    "\n",
    "<img src=\"Figures/rediris.png\" width=\"25%\">\n",
    "\n",
    "\n",
    "### Función de activación softmax\n",
    "\n",
    "Habíamos dicho que cada neurona tiene como función de activación la función sigmoide. Vamos a cambiar esto para la **última, y solo para la última capa**. En la última capa quitamos la función sigmoide y colocamos la función **softmax**. \n",
    "La función softmax es parecida a la sigmoide, ésta también convierte cualquier valor a un valor entre cero y uno. La diferencia está en que la función softmax no trabaja sobre un valor sino sobre un vector. De esta forma, convierte todas las componentes de un vector en valores entre cero y uno, pero, además, garantiza que la suma de todos estos valores sea 1. \n",
    ">La salida de la función softmax conforma una **distribución de probabilidad**.\n",
    "\n",
    "La expresión de la función softmax es:\n",
    "\n",
    "$$ Softmax(\\vec{v})_i = \\frac{e^{v_i}}{\\sum_{j=1}^{n}{e^{v_j}}} $$\n",
    "\n",
    "Veamos esto con un poco de código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# programar función softmax\n",
    "def softmax(i,v):\n",
    "    return np.exp(v[i])/np.sum(np.exp(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([8.34, -2.87, 0.002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = softmax(0,a)\n",
    "S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2 = softmax(1,a)\n",
    "S2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3 = softmax(2,a)\n",
    "S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1+S2+S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de error\n",
    "\n",
    "Nuestra función de error va a comparar las salidas de la red con la etiqueta. Supongamos que nuestra red devuelve (salida del softmax) para una determinada entrada los valores: $o_1=0.78, o_2=0.03, o_3=0,19$, y que su correspondiente etiqueta es: $(1,0,0)$ en codificación softmax. Por tanto, el error, para esta muestra, será: \n",
    "\n",
    "$$error = (1-0.78)^2 + (0-0.03)^2 + (0-0.19)^2 = 0.0854$$\n",
    "\n",
    "Pero, dado que tenemos 150 muestras, la función de error final la calcularíamos sumando los 150 errores particulares de cada muestra.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de Librería de nn\n",
    "\n",
    "Abrimos un archivo de spyder y trabajaremos en lo siguiente:\n",
    "\n",
    "- programar las diferentes funciones de activación\n",
    "- crear la clase Layer\n",
    "- crear la clase Net\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Creación de la función one-hot\n",
    "def one_hot(x, n):\n",
    "    if type(x)==list:\n",
    "        x = np.array(x)\n",
    "    x = x.flatten()\n",
    "    o_h = np.zeros((len(x),n))\n",
    "    o_h[np.arange(len(x)), x]=1\n",
    "    return o_h\n",
    "\n",
    "##----Funciones de activación\n",
    "# programar función softmax\n",
    "def softmax(i,v):\n",
    "    return np.exp(v[i])/np.sum(np.exp(v))\n",
    "\n",
    "#función sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "#función derivada de sigmoide\n",
    "def sigmoid_der(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "#ReLu\n",
    "def relu(x):\n",
    "    if x>=0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def relu_der(x):\n",
    "    if x>=0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def idintity(x):\n",
    "    return x\n",
    "\n",
    "def identity_der():\n",
    "    return 1\n",
    "\n",
    "#TanH\n",
    "def tanh(x):\n",
    "    return 2/(1+np.exp(-2*x)) - 1\n",
    "\n",
    "def tanh_der(x):\n",
    "    return 1 - x**2\n",
    "\n",
    "activ_f = {\n",
    "    'Sigmoid': (sigmoid, sigmoid_der),\n",
    "    'ReLu': (relu, relu_der),\n",
    "    'Identity': (idintity, identity_der),\n",
    "    'Tanh': (tanh, tanh_der)\n",
    "}\n",
    "\n",
    "class Layer:\n",
    "    \"\"\" Layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows, columns, activ_f=activ_f[\"Sigmoid\"]):\n",
    "        self.rows = rows\n",
    "        self.columns = columns + 1  # One column for bias\n",
    "        self.w = Layer.create_w(self.rows, self.columns)\n",
    "        self.W_delta = []\n",
    "        self.b_delta = []\n",
    "        self.activ_f = activ_f[0]\n",
    "        self.o_activ_f_derivate = activ_f[1]\n",
    "        self.error = []\n",
    "        self.o = None\n",
    "\n",
    "    @staticmethod\n",
    "    def create_w(rows, columns):\n",
    "        return np.random.rand(rows, columns) * 0.1\n",
    "\n",
    "    def output(self, x):\n",
    "        x = np.append(x, 1.0)\n",
    "        print('shape x: ',x.shape)\n",
    "        print('shape w: ',self.w.shape)\n",
    "        self.o = self.activ_f(np.dot(self.w, x))\n",
    "        self.o_derivate = self.o_activ_f_derivate(self.o)\n",
    "        return self.o\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Layer_____________\\n     #inputs:\" + repr(self.columns-1) + \"\\n     #neurons: \" + repr(self.rows)\n",
    "\n",
    "\n",
    "class Net:\n",
    "    \"\"\" Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers = []\n",
    "        l_prev = layers[0]\n",
    "        # conección entre las capas\n",
    "        for l in layers[1:]:\n",
    "            self.layers.append(Layer(l, l_prev))\n",
    "            l_prev = l\n",
    "        self.number_of_layers = len(self.layers)\n",
    "\n",
    "    def output(self, x):\n",
    "        o_aux = x\n",
    "        for layer in self.layers:\n",
    "            o_aux = layer.output(o_aux)\n",
    "        return o_aux\n",
    "                       \n",
    "    def train(self, input_data, labels, lr=0.1):\n",
    "        for x, _y in zip(input_data, labels):\n",
    "            y = self.output(x)\n",
    "            error = y - _y\n",
    "            for index_layer in range(self.number_of_layers)[:0:-1]: \n",
    "                error = error * self.layers[index_layer].o_derivate\n",
    "                self.layers[index_layer].W_delta = np.dot(error.reshape((len(error), 1)),\n",
    "                                                          (np.append(self.layers[index_layer-1].o, 1.0))\n",
    "                                                          .reshape(1, len(self.layers[index_layer-1].o)+1))\n",
    "                error = np.dot(error, self.layers[index_layer].w[:,0:-1])\n",
    "            error = error*self.layers[0].o_derivate \n",
    "            self.layers[0].W_delta = np.dot(error.reshape((len(error),1)), np.append(x,1).reshape((1, len(x)+1)))\n",
    "            \n",
    "            for index_layer in range (self.number_of_layers):\n",
    "                #Actualización de los pesos w = w - lr*w_delta  (w_delta son las derivadas parciales)\n",
    "                self.layers[index_layer].w -= self.layers[index_layer].W_delta*lr\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Mini-batch\n",
    "\n",
    "La forma de calcular la función de error que acabamos de ver no se hace así en la práctica. Lo entenderemos mejor con la siguiente comparación.\n",
    "\n",
    "Supongamos que queremos saber la media de altura de todos los habitantes del país. Deberíamos medir a cada persona, sumar todas las alturas y dividir por el número total de personas. Esta es la media exacta, pero nos iba a costar mucho poder llevar a cabo todas estas medidas (somos algo más de 126,2 millones). Otra forma mucho más factible sería tomar una muestra de esta población y calcular la media muestral. Si la muestra es suficientemente variada y amplia obtendremos un valor muy aproximado a la media real pero con mucho menos esfuerzo.\n",
    "\n",
    "Algo parecido podemos aplicar sobre la función de error. Si, en lugar de utilizar todo el conjunto de datos para calcular el error y luego el gradiente que nos lleve a un nuevo conjunto de pesos y umbrales, tomamos una muestra variada de los datos para calcular el error, el gradiente que obtendremos será muy parecido y nos llevará mucho menos tiempo. Por lo tanto, podremos avanzar mucho más rápido con el gradiente descendente.\n",
    "\n",
    "A esta muestra (entiéndase como \"muestra poblacional\", no \"muestra\" como elemento del *dataset*)  que tomamos del *dataset* lo llamaremos **mini-batch**.\n",
    "\n",
    "Por ejemplo, en el caso del *dataset* del iris, son 150 muestras, que, una vez bien “barajadas” para que el *mini-batch* sea variado y representativo, podremos tomar mini-lotes de 15 muestras. Teniendo un total de 10 mini-lotes. Con *datasets* pequeños como éste no se aprecia visiblemente la mejora en velocidad, pero con conjuntos de decenas de miles de muestras o más, la reducción de velocidad es drástica.\n",
    "\n",
    "Por tanto, iremos tomando sucesivos *mini-batchs* de forma iterativa hasta completar el número de estos. Cuando hayamos procesado todos los *mini-batchs* diremos que hemos completado una época (**epoch**), y volveremos a repetir todo el proceso hasta  aproximarnos suficientemente a un mínimo local de la función de error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código\n",
    "\n",
    "Implementemos ya todos estos conceptos. Para ello, utilizaremos el módulo <code>nn</code> que creamos con anterioridad. Este módulo debe incluir una clase denominada <code>Net</code> para implementar redes sencillas de una forma muy simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar dataset de iris\n",
    "data_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creacion del minibatch\n",
    "data_iris = data_iris.values\n",
    "data_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data_iris)\n",
    "X_data = data_iris[:,:4].astype(float)\n",
    "y_data = data_iris[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = one_hot(y_data.astype(int), 3)\n",
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchs_size = 15 #Tamaño del minibartch\n",
    "epochs = 100  #Número de epocas\n",
    "\n",
    "#Crear la red de 4 entradas, 5 en una capa oculta, y 3 en la capa de salida\n",
    "net = Net(layers=[4,5,3])\n",
    "\n",
    "for _ in range(epochs):\n",
    "    for i in range(int(data_iris.shape[0]/batchs_size)):\n",
    "        p = i*batchs_size\n",
    "        net.train(X_data[p:p+batchs_size], y_data[p:p+batchs_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parámetros e hiperparámetros\n",
    "\n",
    "Es posible que, ya a estas alturas, te estés preguntando cómo se decide el número de capas ocultas que debe tener una red, o cuántas neuronas por capa oculta, cómo establecer el tamaño del mini-batch, el número de épocas... Estos y otros muchos, aún por ver, valores en el diseño de la red se denominan **hiperparámetros**. No son valores que la red aprenda por sí sola, sino que le han de ser dados. No hay un procedimiento sistemático para establecerlos, no hay forma de saber *a priori* cuáles son los mejores valores. En muchos casos, no queda más remedio que probar entre diferentes configuraciones y/o seguir algunas recetas, más o menos fiables, provenientes de la experiencia.\n",
    "\n",
    "Al conjunto de todos los pesos (y umbrales, en el caso de que los tengamos diferenciados de los pesos) los denominamos **parámetros** y, a diferencia de los hiperparámetros, sí son aprendidos por la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
