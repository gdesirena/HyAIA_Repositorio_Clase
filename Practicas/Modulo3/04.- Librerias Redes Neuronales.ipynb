{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#000047; padding: 30px; border-radius: 10px; color: white; text-align: center;\">\n",
    "    <img src='Figures/alinco.png' style=\"height: 100px; margin-bottom: 10px;\"/>\n",
    "    <h1> Redes Neuronales Librerías</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- La derivación es el aspecto algorítmicamente más costoso en la utilización de clasificadores basados en redes neuronales. \n",
    "\n",
    "- Calcular las decenas, centenares o centenares de miles de derivadas parciales sería una tarea abrumadora si la tuviéramos que resolver a mano cada vez que construimos una red neuronal. \n",
    "\n",
    "- Afortunadamente contamos con herramientas que hacen eficientemente esa labor por nosotros. **Keras** es una de ellas. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Keras\n",
    "\n",
    "[Keras](https://keras.io/) es una API escrita en Python que nos permite de una forma rápida y cómoda configurar y entrenar redes neuronales.\n",
    "\n",
    "<img src=\"Figures/keras-logo.png\" width=\"30%\">\n",
    "\n",
    "- Hay otra cosa que Keras hace por nosotros y que es sumamente importante: trasladar el cálculo a la GPU en lugar de hacerlo en la CPU. Todo el cálculo que realiza la red para generar una salida es computacionalmente muy alto. Son muchísimas las multiplicaciones y sumas que se llevan a cabo. Pero, afortunadamente, la inmensa mayoría de estas operaciones son paralelizables. Y, de la misma forma que los juegos actuales utilizan la GPU para poder mover rápidamente una inmensa cantidad de puntos, vértices y polígonos, esta misma arquitectura de computación paralela se adapta perfectamente a las necesidades de cálculo de las redes neuronales.\n",
    "\n",
    "- Por supuesto, cuando las redes y sus conjuntos de datos son pequeños no es indispensable disponer de GPU en el ordenador. Pero en cuanto el modelo o los datos crecen el tiempo de cómputo se vuelve crucial.\n",
    "\n",
    "**Keras, a su vez, se apoya sobre otras herramientas como [Tensorflow](https://www.tensorflow.org/) y [CUDA](https://developer.nvidia.com/cuda-zone) (si disponemos de GPU).**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación\n",
    "\n",
    "Visita [https://keras.io/#installation](https://keras.io/#installation) para instalar Keras. Antes debes [instalar Tensorflow](https://www.tensorflow.org/install). \n",
    "\n",
    "**Modo rápido:** Si quieres, puedes realizar una instalación limpia de Keras mediante un entorno virtual (recomendable). \n",
    "\n",
    "**Instala ambiente virtual con Anaconda**\n",
    "\n",
    "Desde consola:\n",
    "\n",
    "<code>conda create -n nombre_de_tu_entorno</code>\n",
    "\n",
    "Crea el entorno virtual\n",
    "\n",
    "<code>conda create -n nombre_de_tu_entorno python=3.6</code>\n",
    "\n",
    "Activa el entorno virtual\n",
    "\n",
    "<code>conda activate nombre_de_tu_entorno<\\code>\n",
    "\n",
    "Dentro del entorno, instala Tensorflow:\n",
    "\n",
    "<code>pip install tensorflow</code>\n",
    "\n",
    "y luego, Keras:\n",
    "\n",
    "<code>pip install keras</code>\n",
    "\n",
    "Con esto tendrás una instalación de Keras para realizar las prácticas, pero sin GPU. Si tu ordenador no tiene GPU, entonces será la opción adecuada. \n",
    "    \n",
    "Podemos instalar todas las librerías que necesitemos como pandas, numpy, matplotlib\n",
    "\n",
    "<code>pip install keras</code>\n",
    "<code>pip install pandas</code>\n",
    "<code>pip install matplotlib</code>\n",
    "\n",
    "**Añadir el Ambiente virtual a Jupyter Notebook**\n",
    "\n",
    "Jupyter Notebook se asegura de que el kernel de IPython esté disponible, pero se debe agregar manualmente un kernel con una versión diferente de Python o un entorno virtual. \n",
    "    \n",
    "-Primero, necesitamos activar su entorno virtual. \n",
    "    \n",
    "-A continuación, instalaremos ipykernel, que proporciona el kernel de IPython para Jupyter:\n",
    "\n",
    "<code>pip install --user ipykernel</code>\n",
    "\n",
    "Ahora se pude añadir un ambiente virtual a jupyter con el siguiente comando:\n",
    "\n",
    "<code>python -m ipykernel install --user --name=myenv</code>\n",
    "\n",
    "Ahora podemos elegir el entorno conda como Kernel en Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "La estructura principal de Keras es el **modelo**, lo cual es una forma de organizar y conectar capas de neuronas. El tipo de modelo más simple es el **modelo secuencial**, que es una pila lineal de capas. Para arquitecturas más complejas, es necesario utilizar la **API funcional** de Keras, que permite crear capas con conexiones arbitrarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos nuestro modelo\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora apilaremos capas con <code>.add()</code>. Keras denomina a las capas neuronales básicas como **densas** <code>Dense</code>, lo que significa que todas las entradas son conectadas a todas las neuronas. Como observamos en la figura siguiente, todas las $n$ entradas se conectan a todas las $m$ neuronas. Veremos más adelante que esto no siempre es así. Hay capas denominadas **convolutivas** que no siguen este patrón de conexión, sino que parte de las entradas se conectan solo a algunas neuronas de la capa. Pero, por ahora, eso es otra historia.\n",
    "\n",
    "<img src=\"Figures/densa.svg\" width=\"30%\">\n",
    "\n",
    "Fíjate que en la instanciación de la primera capa densa tenemos que especificar el número de entradas <code>input_dim=4</code>, pero en la siguiente capa no. Keras sabe que son $5$ entradas puesto que en la capa anterior hay $5$ neuronas <code>units=5</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importar dense\n",
    "from tensorflow.keras.layers import Dense\n",
    "# capa con 4 entradas\n",
    "model.add(Dense(input_dim = 4, units = 5, activation = 'sigmoid')) #4 ntradas conectadas a 5 neuronas\n",
    "# capa con 5 neuronas\n",
    "model.add(Dense(units = 3, activation = 'softmax')) #5 ntradas conectadas a 3 neuronas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuración del entrenamiento\n",
    "\n",
    "Una vez configurado el modelo, especificaremos el proceso de aprendizaje.\n",
    "\n",
    "- La **función de error** o **pérdida** (**loss**) que utilizaremos es <code>loss='mse'</code>, lo que significa *mean squared error* o error cuadrático medio. Es parecida a la suma de todos los errores que ya hemos visto pero dividido por el número de muestras sobre las que calculamos el error. \n",
    "\n",
    "- En cuando al optimizador, usaremos el clásico gradiente descendente <code>optimizer=keras.optimizers.SGD(lr=1)</code> (stochastic gradient descent) al que le aplicamos un *learning rate* o **tasa de aprendizaje** de $1$. \n",
    "\n",
    "\\- Lo de *gradiante descendente*, ya lo sabemos. Pero, ¿lo de *estocástico* qué significa? \n",
    "\n",
    "En este contexto, **estocástico** significa que no vamos a calcular el error sobre todo el conjunto de muestras en cada iteración sino sobre un subconjunto **aleatorio** de ellos. Ese subconjunto corresponde al *mini-batch* que ya vimos.\n",
    "\n",
    "Durante el proceso de entrenamiento vamos progresivamente descendiendo por la función de error hasta llegar a algún mínimo. Para verificar que vamos avanzando adecuadamente es conveniente ir comprobando cómo el valor de la función de error va bajando (*loss*). Sin embargo, *loss* es un valor que no indica nada en sí, solo que si baja es buena señal. Pero, lo que realmente nos va diciendo cuánto vamos mejorando es el *accuracy* (precisión) <code>metrics=['accuracy']</code>. Este valor se calcula introduciendo las muestras en la red y comprobando qué porcentaje de ellas ha clasificado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilar el modelo\n",
    "model.compile(loss='mse', optimizer=keras.optimizers.SGD(learning_rate=0.1), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparación de los datos. Entrenamiento y test\n",
    "\n",
    "Una vez completamente definido el modelo y cómo lo vamos a entrenar es necesario preparar los datos sobre los que vamos a trabajar. Esta vez vamos a dividir nuestro conjunto de muestras en dos subconjuntos: uno  para **entrenar** (*train*) y otro para **verificar** (*test*). Cuando entrenamos una red neuronal con un conjunto de muestras, ¿cómo podemos estar seguros de que esa red es capaz de clasificar correctamente nuevas muestras que nunca haya visto antes? Dicho de otro modo, ¿cómo podemos saber si la red puede **generalizar**?\n",
    "\n",
    "Para saber si una red ha aprendido correctamente hacemos esta división. Vamos a entrenar la red con el **conjunto de entrenamiento**. Y una vez entrenada comprobaremos cuántas muestras del **conjunto de test** es capaz de clasificar correctamente. Si el porcentaje de aciertos es satisfactorio concluimos que la red está correctamente entrenada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iris = pd.read_csv('Data/iris.data', header=None)\n",
    "data_iris['out'] = data_iris.iloc[:,4].astype('category').cat.codes\n",
    "\n",
    "data_iris['out'].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3               4  out\n",
       "0    5.1  3.5  1.4  0.2     Iris-setosa    0\n",
       "1    4.9  3.0  1.4  0.2     Iris-setosa    0\n",
       "2    4.7  3.2  1.3  0.2     Iris-setosa    0\n",
       "3    4.6  3.1  1.5  0.2     Iris-setosa    0\n",
       "4    5.0  3.6  1.4  0.2     Iris-setosa    0\n",
       "..   ...  ...  ...  ...             ...  ...\n",
       "145  6.7  3.0  5.2  2.3  Iris-virginica    2\n",
       "146  6.3  2.5  5.0  1.9  Iris-virginica    2\n",
       "147  6.5  3.0  5.2  2.0  Iris-virginica    2\n",
       "148  6.2  3.4  5.4  2.3  Iris-virginica    2\n",
       "149  5.9  3.0  5.1  1.8  Iris-virginica    2\n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def one_hot(x, n):\n",
    "    if type(x)==list:\n",
    "        x = np.array(x)\n",
    "    x = x.flatten()\n",
    "    o_h = np.zeros((len(x), n))\n",
    "    o_h[np.arange(len(x)), x] = 1\n",
    "    return o_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iris = data_iris.values\n",
    "np.random.shuffle(data_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = data_iris[:,:4].astype(float)\n",
    "y_data = one_hot(data_iris[:,-1].astype(int), n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Particionando la data Tomamos el 80% (120) de las muestras para entrenar y el 20% (30) para testear\n",
    "X_train = X_data[:120]\n",
    "y_train = y_data[:120]\n",
    "\n",
    "X_test = X_data[120:]\n",
    "y_test = y_data[120:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Procedemos a hacer el entrenamiento de la red. Para ello solo tenemos que invocar al método <code>fit</code> del modelo y especificarle que queremos entrenar $200$ épocas y que use un tamaño de lote de $15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2352 - accuracy: 0.3417\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.3417\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2239 - accuracy: 0.3417\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 932us/step - loss: 0.2206 - accuracy: 0.3417\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2179 - accuracy: 0.3417\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.3417\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.2137 - accuracy: 0.3417\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.3417\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 930us/step - loss: 0.2094 - accuracy: 0.3417\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.3417\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.3417\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.3417\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1982 - accuracy: 0.3417\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.3417\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1891 - accuracy: 0.4250\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.5917\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 757us/step - loss: 0.1805 - accuracy: 0.6000\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.6750\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 932us/step - loss: 0.1740 - accuracy: 0.6750\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 858us/step - loss: 0.1714 - accuracy: 0.6833\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.6833\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.6833\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.1650 - accuracy: 0.6833\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.1631 - accuracy: 0.6833\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.6833\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.6833\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.6833\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1564 - accuracy: 0.6833\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1550 - accuracy: 0.6833\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1534 - accuracy: 0.6833\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1522 - accuracy: 0.6833\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1507 - accuracy: 0.6833\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1495 - accuracy: 0.6833\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1482 - accuracy: 0.6833\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1471 - accuracy: 0.6833\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1458 - accuracy: 0.6833\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.6833\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.6833\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 502us/step - loss: 0.1425 - accuracy: 0.6833\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.1415 - accuracy: 0.6833\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.1406 - accuracy: 0.6833\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.6833\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1386 - accuracy: 0.6833\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1375 - accuracy: 0.6833\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - ETA: 0s - loss: 0.1254 - accuracy: 0.86 - 0s 2ms/step - loss: 0.1367 - accuracy: 0.6833\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.6833\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.1349 - accuracy: 0.6833\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1342 - accuracy: 0.6833\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.6833\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 707us/step - loss: 0.1328 - accuracy: 0.6833\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.1320 - accuracy: 0.6833\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 715us/step - loss: 0.1314 - accuracy: 0.6833\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 1000us/step - loss: 0.1305 - accuracy: 0.6833\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.1298 - accuracy: 0.6833\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 788us/step - loss: 0.1291 - accuracy: 0.6833\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.6833\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 858us/step - loss: 0.1281 - accuracy: 0.6833\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 1000us/step - loss: 0.1273 - accuracy: 0.6833\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.6833\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.6833\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 788us/step - loss: 0.1257 - accuracy: 0.6833\n",
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.6833\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.6833\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 715us/step - loss: 0.1241 - accuracy: 0.6833\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 786us/step - loss: 0.1236 - accuracy: 0.6833\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.1232 - accuracy: 0.6833\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.6833\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.6833\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.6833\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 897us/step - loss: 0.1212 - accuracy: 0.6833\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 788us/step - loss: 0.1209 - accuracy: 0.6833\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 774us/step - loss: 0.1204 - accuracy: 0.6833\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 932us/step - loss: 0.1201 - accuracy: 0.6833\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 765us/step - loss: 0.1197 - accuracy: 0.6833\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 715us/step - loss: 0.1193 - accuracy: 0.6833\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.1190 - accuracy: 0.6917\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.6917\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.1181 - accuracy: 0.6833\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.1177 - accuracy: 0.6917\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.1173 - accuracy: 0.6917\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.6917\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.1167 - accuracy: 0.6917\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 713us/step - loss: 0.1163 - accuracy: 0.6917\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 790us/step - loss: 0.1162 - accuracy: 0.6917\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.1157 - accuracy: 0.7000\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 603us/step - loss: 0.1153 - accuracy: 0.6917\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 933us/step - loss: 0.1150 - accuracy: 0.6917\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.6917\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 915us/step - loss: 0.1142 - accuracy: 0.7000\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 645us/step - loss: 0.1139 - accuracy: 0.6917\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 786us/step - loss: 0.1138 - accuracy: 0.7083\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.1133 - accuracy: 0.7000\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 866us/step - loss: 0.1133 - accuracy: 0.7000\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.1126 - accuracy: 0.7167\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.7167\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.1124 - accuracy: 0.7250\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.7250\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.7167\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.7167\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 777us/step - loss: 0.1110 - accuracy: 0.7167\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.7250\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.1104 - accuracy: 0.7167\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.1101 - accuracy: 0.7167\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.7167\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.1095 - accuracy: 0.7167\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.7583\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 930us/step - loss: 0.1088 - accuracy: 0.7167\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.7167\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.1085 - accuracy: 0.7667\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 873us/step - loss: 0.1081 - accuracy: 0.7750\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.7250\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 903us/step - loss: 0.1075 - accuracy: 0.7500\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 858us/step - loss: 0.1073 - accuracy: 0.7333\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.7583\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.7583\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.7417\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.7417\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.1058 - accuracy: 0.7833\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.7583\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.8083\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 787us/step - loss: 0.1049 - accuracy: 0.7917\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.7667\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 933us/step - loss: 0.1046 - accuracy: 0.7750\n",
      "Epoch 124/200\n",
      "8/8 [==============================] - 0s 725us/step - loss: 0.1040 - accuracy: 0.8333\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 790us/step - loss: 0.1040 - accuracy: 0.8083\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 933us/step - loss: 0.1037 - accuracy: 0.8667\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 647us/step - loss: 0.1032 - accuracy: 0.8333\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.8083\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1028 - accuracy: 0.8083\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.1025 - accuracy: 0.8583\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.1022 - accuracy: 0.8417\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1020 - accuracy: 0.8667\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 793us/step - loss: 0.1015 - accuracy: 0.8500\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 895us/step - loss: 0.1012 - accuracy: 0.8333\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.8750\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.8500\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 871us/step - loss: 0.1002 - accuracy: 0.8667\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.8750\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.8833\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.8833\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 724us/step - loss: 0.0990 - accuracy: 0.8667\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.0988 - accuracy: 0.8417\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 787us/step - loss: 0.0987 - accuracy: 0.8833\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 917us/step - loss: 0.0984 - accuracy: 0.8833\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9083\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 789us/step - loss: 0.0976 - accuracy: 0.8667\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.0975 - accuracy: 0.8750\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.0969 - accuracy: 0.8917\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.0967 - accuracy: 0.8917\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 0s 999us/step - loss: 0.0964 - accuracy: 0.9167\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.0961 - accuracy: 0.8750\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.0960 - accuracy: 0.8917\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.8833\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9000\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 0s 769us/step - loss: 0.0953 - accuracy: 0.8833\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.8833\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.0943 - accuracy: 0.8917\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9167\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.8917\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.0930 - accuracy: 0.9000\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.8917\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 0s 886us/step - loss: 0.0928 - accuracy: 0.9167\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 0s 932us/step - loss: 0.0923 - accuracy: 0.9083\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9167\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 0s 1000us/step - loss: 0.0919 - accuracy: 0.8917\n",
      "Epoch 166/200\n",
      "8/8 [==============================] - 0s 939us/step - loss: 0.0914 - accuracy: 0.9083\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9167\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9000\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9250\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0900 - accuracy: 0.9083\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 0s 864us/step - loss: 0.0896 - accuracy: 0.9167\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.9167\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9167\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 0s 859us/step - loss: 0.0890 - accuracy: 0.9167\n",
      "Epoch 175/200\n",
      "8/8 [==============================] - 0s 859us/step - loss: 0.0885 - accuracy: 0.9167\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9417\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.0877 - accuracy: 0.9167\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 0s 715us/step - loss: 0.0873 - accuracy: 0.9250\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9083\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9417\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 0s 858us/step - loss: 0.0862 - accuracy: 0.9250\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9417\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.0854 - accuracy: 0.9083\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.0854 - accuracy: 0.9333\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.0849 - accuracy: 0.9333\n",
      "Epoch 186/200\n",
      "8/8 [==============================] - 0s 714us/step - loss: 0.0843 - accuracy: 0.9500\n",
      "Epoch 187/200\n",
      "8/8 [==============================] - 0s 571us/step - loss: 0.0840 - accuracy: 0.9250\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 0s 858us/step - loss: 0.0842 - accuracy: 0.9167\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9167\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.9500\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 0s 878us/step - loss: 0.0828 - accuracy: 0.9333\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9500\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9583\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9167\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9417\n",
      "Epoch 196/200\n",
      "8/8 [==============================] - 0s 930us/step - loss: 0.0815 - accuracy: 0.9667\n",
      "Epoch 197/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9250\n",
      "Epoch 198/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9333\n",
      "Epoch 199/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9417\n",
      "Epoch 200/200\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9417\n"
     ]
    }
   ],
   "source": [
    "# fit del modelo\n",
    "history = model.fit(X_train, y_train, epochs=200, batch_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización\n",
    "\n",
    "El objeto <code>history</code> que nos devuelve el método <code>fit</code> contiene la información acerca del progreso del entrenamiento. Vemos cómo el valor de *loss* va decreciendo mientras el *accuracy* va aproximándose a $1$, lo que representa casi el 100% de las muestras de entramiento bien clasificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualización de los datos\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "Durante el entrenamiento *accuracy* nos indica el porcentaje de aciertos sobre el mismo conjunto de entrenamiento. Pero nos interesa conocer el porcentaje de acierto sobre un conjunto no visto antes por la red. Para ello utilizamos el conjunto de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 128ms/step - loss: 0.0840 - accuracy: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08397707343101501, 0.9666666388511658]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test de la clasificación\n",
    "model.evaluate(X_test, y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicción\n",
    "\n",
    "Una vez entrenada y testeada la red, podemos ponerla en producción. Keras tiene funciones para guardar tanto el modelo como los pesos ya entrenados. Si queremos hacer una clasifiación invocaremos el método <code>predict</code> del modelo.\n",
    "\n",
    "Vamos a ver qué resultados nos ofrece la red si introducimos el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(4, 5) dtype=float32, numpy=\n",
       " array([[ 6.8700105e-01,  6.9894445e-01,  7.5015172e-02, -3.0223516e-01,\n",
       "          9.2542417e-02],\n",
       "        [ 1.6111529e-01,  5.8714420e-01,  8.7359577e-01, -5.0393778e-01,\n",
       "          1.4185227e+00],\n",
       "        [-9.4797736e-01,  4.6136832e-01, -1.2037786e+00,  9.5819765e-01,\n",
       "         -1.0442647e+00],\n",
       "        [-4.8261967e-01,  8.1632006e-01, -1.4108422e-01,  1.2685475e-04,\n",
       "         -6.9331062e-01]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32, numpy=\n",
       " array([ 0.2143387 ,  0.00032961,  0.14621101, -0.14152426,  0.25219774],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_1/kernel:0' shape=(5, 3) dtype=float32, numpy=\n",
       " array([[ 0.6527532 ,  0.76274925, -0.8929468 ],\n",
       "        [ 0.02700424,  0.5329162 ,  0.56069386],\n",
       "        [ 1.6745051 , -0.23074572, -0.39691043],\n",
       "        [-1.4722273 , -0.3566309 ,  0.71018434],\n",
       "        [ 1.7188735 , -0.5103529 , -1.8819418 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(3,) dtype=float32, numpy=array([-0.78332704,  0.25843418,  0.5248933 ], dtype=float32)>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28241774, 0.5177517 , 0.19983055],\n",
       "       [0.85832024, 0.13144942, 0.01023047],\n",
       "       [0.03999015, 0.34606633, 0.6139435 ],\n",
       "       [0.881958  , 0.1101514 , 0.00789055],\n",
       "       [0.03481514, 0.32071146, 0.6444734 ],\n",
       "       [0.04367683, 0.36635798, 0.5899652 ],\n",
       "       [0.8842126 , 0.10818891, 0.00759855],\n",
       "       [0.15157117, 0.5402334 , 0.30819544],\n",
       "       [0.14093994, 0.52559584, 0.3334642 ],\n",
       "       [0.87347424, 0.1178942 , 0.00863157],\n",
       "       [0.03718086, 0.3491342 , 0.6136849 ],\n",
       "       [0.1795645 , 0.5135569 , 0.30687854],\n",
       "       [0.88721263, 0.10569625, 0.00709102],\n",
       "       [0.02956938, 0.30803463, 0.662396  ],\n",
       "       [0.04991969, 0.3711172 , 0.5789631 ],\n",
       "       [0.10048655, 0.47935638, 0.42015707],\n",
       "       [0.11744903, 0.51432896, 0.36822197],\n",
       "       [0.09202661, 0.48264468, 0.4253287 ],\n",
       "       [0.03026407, 0.31206495, 0.657671  ],\n",
       "       [0.8836737 , 0.10865405, 0.00767213],\n",
       "       [0.04050218, 0.35507575, 0.6044221 ],\n",
       "       [0.14702736, 0.5143498 , 0.33862284],\n",
       "       [0.8735601 , 0.11756378, 0.00887609],\n",
       "       [0.8854301 , 0.10703292, 0.00753697],\n",
       "       [0.08532212, 0.46874747, 0.44593045],\n",
       "       [0.02693316, 0.2814509 , 0.69161594],\n",
       "       [0.10715452, 0.48618957, 0.40665594],\n",
       "       [0.11336213, 0.497218  , 0.38941985],\n",
       "       [0.844136  , 0.14411394, 0.01175016],\n",
       "       [0.01961373, 0.25791267, 0.7224736 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicción \n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 3 Redes neuronales\n",
    "\n",
    "- Varía algunos hiperparámetros (learning rate, tamaño del mini-lote, número de neuronas en la capa oculta, número de épocas) y observa qué ocurre.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
